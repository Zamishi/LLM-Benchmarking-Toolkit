# LLM-Benchmarking-Toolkit

**Python | PyQt5 | SymPy | OpenAI API | Gemini API**

---

## Overview

The **LLM Benchmarking Toolkit** is a Python-based research tool designed to **analyze and benchmark multiple large language models (LLMs)** such as OpenAI and Gemini. It supports **mathematical, coding, and textual problem-solving tasks** by integrating OCR and symbolic computation, providing a research-focused platform for **performance evaluation, comparison, and analysis**.

---

## Key Features

- **Dual LLM Comparison:** Evaluate how different LLMs perform on the same input.  
- **OCR Integration:** Extract text from images or screenshots for processing.  
- **Symbolic Computation:** Use SymPy for accurate mathematical problem handling.  
- **Research Interface:** Streamlined overlay interface to monitor and collect results efficiently.  
- **Extensible Framework:** Add new LLMs, datasets, or evaluation metrics as needed.

---

## Installation

1. Clone the repository:  
```bash
git clone https://github.com/yourusername/llm-benchmarking-toolkit.git
cd llm-benchmarking-toolkit
````

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Ensure **Tesseract OCR** is installed and accessible in your PATH.

* Windows default: `C:\Program Files\Tesseract-OCR\tesseract.exe`
* Mac/Linux: install via your package manager.

4. Set your API keys (optional environment variables):

```bash
export OPENAI_API_KEY="your_openai_api_key"
export GEMINI_API_KEY="your_gemini_api_key"
```

---

## Usage

Run the main interface:

```bash
python main.py --debug
```

Optional flags:

* `--no-openai` : Disable OpenAI LLM
* `--no-gemini` : Disable Gemini LLM
* `--debug` : Save debug images and logs for research analysis

---

## Research Output

### Math Problem Recognition
The screenshots below show OCR-based extraction of math problems, followed by solutions generated by the integrated LLMs.

**SymPy + LLM Output Comparison**

- **OpenAI GPT:** Provides step-by-step symbolic solutions and final numeric answers.  
- **Gemini:** Provides alternate reasoning and solutions for the same problem.  

<img width="1373" height="454" alt="image" src="https://github.com/user-attachments/assets/1111490a-7c57-4193-b875-d81a42653d9f" />
<img width="1537" height="1049" alt="Screenshot 2025-09-26 212710" src="https://github.com/user-attachments/assets/8fdc655f-8734-4e2c-8d87-ae6f9d673d08" />

### Textual Analysis
The screenshots below demonstrate textual analysis tasks, including summarization, Q&A, and semantic scoring.

**LLM Performance on Textual Input**

- **OpenAI GPT:** Generates concise summaries and high-quality semantic answers.  
- **Gemini:** Produces alternate phrasing and additional contextual insights.  

<img width="1578" height="508" alt="Screenshot 2025-09-26 213959" src="https://github.com/user-attachments/assets/2b25d5ee-4d50-4b83-83aa-aca75620d02c" />
<img width="876" height="587" alt="Screenshot 2025-09-26 213750" src="https://github.com/user-attachments/assets/aadc8190-8b30-4f20-986c-47961891319d" />

---

### API Keys
This toolkit requires API keys to interact with the LLMs. Users must provide their own:  

export OPENAI_API_KEY="your_openai_api_key"
export GEMINI_API_KEY="your_gemini_api_key"

**Note: The project is intended purely for research, benchmarking, and learning purposes.




 
